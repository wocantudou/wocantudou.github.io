![VLA](BigModel/VLA/VLA.png)
# 视觉-语言-动作 (VLA) 技术概述

## 核心架构

VLA 系统通常采用“视觉-语言模型（VLM）+ 动作预测模块” 的架构。视觉-语言模型部分使用预训练的视觉编码器（如ViT、DINOv2、SigLIP等）提取图像帧或视频特征，并使用大型语言模型（如LLaMA/GPT变体）对自然语言指令/描述进行编码。两种模态的特征一般通过**多模态融合**机制（如交叉注意力、特征拼接或统一 token 形式）整合为统一表征。随后，动作预测模块以这个多模态上下文为条件，生成对应的动作输出。例如，SmolVLA 将多视角图像、语言指令和传感器状态编码为“前缀 token”，然后一个流匹配 (flow-matching) 变换器根据这些前缀连续输出机器人动作序列。早期工作（如 Google DeepMind 的 RT-2）也采用将离散化的机器人动作表示为 token 的方式进行统一学习。这种端到端 Transformers 架构赋予模型在同一空间内同时理解视觉场景、语言意图和动作规划的能力。值得一提的是，也有研究探索**多模态 VAE**架构作为轻量替代方案，用于视觉-语言与动作的对齐学习。

以 CLIPort 为例，其**双流架构**同时利用语义通路与空间通路进行感知与推理。语义通路使用预训练的 CLIP（ResNet）提取图像语义，并在解码阶段融合语言指令；空间通路并行处理 RGB-D 输入保持空间精度。两路特征在多层级上进行融合，最终输出每个像素的抓取/放置亲和度图。该设计将**CLIP 强大的视觉-语言对比预训练**（捕获物体类别、属性等概念）与 Transformer 端到端动作规划的能力结合起来。类似的双流或联合 Transformer 架构在多种 VLA 模型中被采用，用以捕获不同模态间的互补信息。

## 动作建模

VLA 中的动作输出形式多样，取决于任务需求：

* **离散动作 token**：许多模型（如 RT-2、OpenVLA）将机器人运动原语离散化为 token 序列。例如，RT-2 将每个控制步骤用一串“字符串命令”表示（包含位移、旋转、夹爪状态等），并将其作为 Transformer 的输出序列。这种方式使模型可像生成文本一样生成动作，有利于微调现有视觉-语言模型。
* **连续轨迹/动作块**：另一些方法直接输出连续控制信号（如关节角度、末端执行器位置）。例如 SmolVLA 的动作专家生成“动作块”——一段连续的未来动作序列，并通过**流匹配 (flow-matching)** 损失进行训练，使模型能**非自回归**地预测连续动作。与离散 token 需要逐步解码不同，流匹配可同时预测完整轨迹，提高实时控制效率。
* **高层轨迹/路径**：在导航或运动规划任务中，动作也可以表示为连续路径或高层运动目标（如中间航路点）。这类模型通常以高频感知数据（如 LiDAR、GPS）和语言指令为输入，输出空间位置或轨迹点。
* **其他表征**：某些任务（如人体动作预测）会使用骨架关键点序列作为输出；工业任务中则可能用规划器输出组装步骤。然而，对于机器人的话题主要关注关节/末端执行器控制信号。总之，VLA 模型的动作建模既包括符号化动作 token，也包括连续控制输出，这取决于具体任务与架构选择。

## 数据集

训练和评估 VLA 系统常用以下多模态数据集：

* **Ego4D**：Meta 发布的大规模第一人称视角视频数据集（3025 小时），涵盖日常活动中手部交互、对话、预测等多种任务。如预测穿戴者下一步动作、对象交互等。
* **EPIC-KITCHENS**：烹饪类第一人称视频集（100 小时），包含大量逐帧标注的行动（切割、移动物体等）和对应字幕，适用于动作识别和语言-动作学习。
* **Something-Something**：短视频动作数据集（百万级视频），每条视频附有人类用自然语言描述的动作标签，如“用手推开物体”，用于学习视觉-动作语义映射。
* **Charades-STA**：室内日常活动视频（约1万条）集，附带自然语言句子对应的行为发生时间标注，支持视频中的语言查询与时序动作定位。
* **COIN/YouCook2/HowTo100M** 等：包含烹饪、日常任务的大规模视频-文本对，用于学习更通用的视觉-语言表征和任务规划。
* **仿真与机器人演示集**：如 Open X-Embodiment（包含百万级真实机器人轨迹数据）、Google Robotics Transformer (RoboTransformer) 等，用于直接训练视觉-动作映射模型。
* **导航与多模态交互数据集**：如家庭或自动驾驶场景下的指令导航集、图像问答集等（例如CoVLA数据集包含80+小时驾驶视频与文本指令）。

这些数据集为 VLA 模型提供了跨模态的监督信息，使其能够从视觉-语言-动作三者联合分布中学习。

## 跨模态对齐技术

为了让模型理解视觉内容与语言描述之间的对应关系，VLA 采用了多种对齐与融合方法：

* **对比学习预训练**：利用如 CLIP、SigLIP 等视觉-语言对比模型，将图像和文本投射到同一语义空间。这种预训练使得视觉编码器能输出与语言编码可比的特征，作为 VLA 上游编码器的初始化。
* **交叉注意力融合**：在 Transformer 模块中，引入视觉-语言交叉注意力机制，将语言信息作为查询对视觉特征进行加权组合（或反之）。这样模型可以根据指令关注图像中对应区域，实现语义层面的融合。例如，前缀 token 技术将视觉特征和语言编码都转换为 token 序列并在注意力层中联合编码，使模型在同一语义空间推理。
* **共享表示学习**：通过多任务或联合训练策略，鼓励模型学习对齐的表示。例如，将场景描述和后续动作联合预测，使模型内部隐空间捕获了“看到**什么**（视觉）和需要做**什么**（语言）”之间的关联。这可以理解为在端到端训练过程中自动完成了跨模态语义对齐。
* **多模态编码器**：有的工作直接使用预训练的多模态编码器（如 Flamingo、PaLM-E 中的视觉编码）将视觉和语言融合，这些模型在大规模图文数据上已学习到内在对齐信息。
  总之，VLA 系统通过对比学习、联合编码以及 Transformer 的注意力机制来实现视觉、语言和行为意图之间的语义对齐和深度融合。

## 训练方法

VLA 模型的训练通常借鉴大模型的训练范式，并结合模拟或实景数据：

* **预训练+微调**：类似大语言模型，先在大规模多模态数据（如图像-文本、视频-文本对）上预训练视觉-语言组件，然后在具体的机器人轨迹或任务数据上微调。这种方式被PaLM-E、SmolVLA等采用。例如，PaLM-E 在大规模语言、视觉和模拟机器人任务上联合训练，提高了跨任务泛化能力；SmolVLA 在通用动作数据上预训练，再针对特定操作任务进行后续训练。
* **对比学习**：使用图文对比损失（如 CLIP 风格的 InfoNCE）来对齐视觉和语言表示，为下游动作预测提供语义先验。这通常发生在预训练阶段，确保视觉编码器和语言编码器学习一致的特征。
* **联合多模态训练**：同时训练视觉、语言、动作模块，以多任务损失学习共融表示。例如，一些方法在同一个模型中联合优化视频理解和动作模仿任务，使模型从更多样本中获益。
* **模态蒸馏**：将大模型的知识蒸馏到轻量级模型，以适配嵌入式设备。一些工作（如 SigLIP 等）采用自蒸馏和掩码预测来提升视觉-语言对齐，并将知识转移到下游任务。
* **强化与模仿学习**：在具备交互反馈的场景，可利用模仿学习（行为克隆）或强化学习对策略进行微调。例如 RT-2 使用离线示例进行模仿学习以训练动作策略，而 ORION 等自驾系统则在自动驾驶模拟环境中使用强化学习提升策略表现。
  这些训练策略常常结合使用：如先用对比学习预训练视觉语言模块，再用示例数据微调动作头，最后可能再通过强化学习优化控制性能。多模态模型往往需要精心设计的训练流水线，以平衡语义理解与控制精度。

## 代表性模型或系统

当前已有多种领先的 VLA 模型与系统，体现了不同的技术路线和创新：

* **CLIPort (2021)**：一个早期端到端视觉-语言-动作模型，用于桌面物体搬运任务。CLIPort 在操作流水线上采用双流架构：一条流水使用冻结的 CLIP 提取语义，另一条流水使用卷积网络提取空间信息，最后输出抓取/放置动作。
* **VIMA (2022)**：使用 Transformer 模型同时处理物体中心视觉 token 和指令 token，通过自然语言快速泛化到新空间推理任务。
* **MERLOT Reserve (2022)**：一个面向视频理解的多模态模型，联合学习视频帧、字幕和音频，通过“遮挡式”预训练在 2000 万视频上学习时序语义表示。其主攻视频问答和常识推理任务，对多模态“脚本知识”学习有启发意义。
* **VideoGPT (2021)**：基于 VQ-VAE+Transformer 的视频生成模型。虽然主要用于视频生成，但可扩展为条件生成（如机器人推杆实验中的动作控制），展示了视觉与动作数据联合建模的可能性。
* **RT-2 (2023)**：Google DeepMind 提出，将离散化的机器人控制命令转为文本 token，连接预训练的 VLM（如 PaLM-E、LLaMA）与机器人示例，实现“语言即程序”式的命令生成。RT-2 可零样本执行指令并在复杂操作中表现优异。
* **PaLM-E (2023)**：Google 提出的大型视觉-语言-实体模型，将视觉和传感器输入直接插入语言模型，通过端到端训练支持多种机器人任务。PaLM-E 在视觉问答、机器人规划等任务中展现了通用能力，多种模态的联合训练使其具有跨任务迁移能力。
* **OpenVLA (2023)**：一款开源的 VLA 模型，由 MIT 等提出。它在近百万级真实机器人演示数据（Open X-Embodiment）上训练，采用 Llama-2 语言模型和 DINOv2/SigLIP 视觉编码器，示范了开放社区可访问的 VLA 训练范式。
* **SmolVLA (2023)**：Intel 等提出的轻量 VLA 模型，通过使用小型 VLM（如 SigLIP+DINOv2 和小型 LM）及流匹配动作头，实现低延迟推理并匹配大型模型性能。其训练遵循预训练-后训练的思路，用数万示例就能逼近大模型效果。
* **EdgeVLA (2024)**：K-Scale 实验室提出的高效 VLA 模型，专为边缘设备优化。其架构使用 0.5B 的 Qwen2 语言模型，视觉前端结合 SigLIP 和 DINOv2，并采用非自回归输出策略。该模型在 120 万图文混合数据上预训练后，在桥梁数据集上的性能接近原始 7.5B OpenVLA。
* **Helix (2025)**：Figure AI 推出的全身人形机器人控制系统，采用集成的 VLA 模型进行全身操控。其架构与上述相似：视觉输入由 ViT/DINOv2 处理，指令由 LLM（Llama-2/GPT）处理，融合后生成离散动作 token。Helix 的创新在于高频率人形机器人控制与广泛的多任务示例。
* **DexVLA (2025)**：近期学术工作，针对长序列、多机器人体型场景提出。它使用1B参数的**扩散式动作专家**，并采用跨机器人逐步预训练策略，使其对多种机器人（单臂、双臂、机械手）具备自适应能力。实验显示 DexVLA 在复杂任务上优于现有 Octo、OpenVLA 和扩散策略等模型。
* **EgoVLA (2024, CVPR 提交)**：预印本提出从大规模第一人称人类活动视频中学习 VLA 模型。虽然尚未正式发表，但其代表了将现有视-言-动作技术向拟人视角扩展的趋势。

以上模型覆盖了从桌面操作到自动驾驶、人形机器人等多个领域，体现了 VLA 方法在架构设计（双流 vs 统一）、动作输出（离散 token vs 连续预测）、训练策略（大模型 vs 轻量化）的多样性。

## 应用场景

视觉-语言-动作技术已在多种任务中展现潜力，包括但不限于：

* **视频问答与检索**：如 Ego4D、TVQA 等多模态视频问答任务，系统需要理解视频内容并用自然语言回答；视频检索任务中，用语言查询视频片段或根据视频寻找相关文本。
* **语言指导的动作预测**：给定当前视频帧和指令，预测未来动作或运动轨迹，如预测穿戴者下一步行为、根据描述预测机器人完成物体操作路径。
* **指令执行与导航**：机器人语义导航与自动驾驶场景中，VLA 模型能将语言指令（“到加油站后第二个路口右转”）与视觉环境联系起来，规划移动或操控指令。例如自驾领域出现了CoVLA、OpenDriveVLA、ORION 等系统，能够在真实驾驶视频和传感器数据上理解导航指令并生成控制行为。
* **机器人操作与物体交互**：如桌面物体搬运、装配任务等，模型根据语言指令操纵机械臂完成抓取、放置、装配等操作。在工业环境中，VLA 让机器人能够通过自然语言学习新装配任务（“将螺丝拧紧在红色模块上”）并执行。
* **人机交互与辅具**：智能辅具或协作机器人可利用 VLA 理解口头指令并辅助人类，如外科手术助手根据语音命令递送器械。
* **多模态时间序列分析**：包括动作检测、人体骨架预测等，结合视觉和描述信息提高预测精度。如将动作语言描述和视频帧联合输入模型，以预测序列骨架姿态。
  综上，VLA 技术在**机器人导航、动作预测、多模态检索、视频理解**等多领域均有广泛应用，正在推动从“感知-规划-控制”向更直观高层语义融合的智能系统发展。

**参考资料：** 本文所述内容主要参考最新文献和项目，包括 MERLOT Reserve、RT-2、OpenVLA、PaLM-E、SmolVLA等，以及相关概述性论文。各类数据集信息参见 Ego4D、EPIC-KITCHENS、Charades-STA等来源。
